{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучите модель классифицировать комментарии на позитивные и негативные. В вашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Постройте модель со значением метрики качества *F1* не меньше 0.75. \n",
    "\n",
    "**Инструкция по выполнению проекта**\n",
    "\n",
    "1. Загрузите и подготовьте данные.\n",
    "2. Обучите разные модели. \n",
    "3. Сделайте выводы.\n",
    "\n",
    "Для выполнения проекта применять *BERT* необязательно, но вы можете попробовать.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pymystem3 import Mystem\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import torch\n",
    "import transformers\n",
    "from tqdm import notebook\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "from lightgbm import LGBMClassifier, LGBMRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk import WordNetLemmatizer, word_tokenize\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from pymorphy2 import MorphAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/datasets//toxic_comments.csv\")\n",
    "df.head(5)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text     0\n",
       "toxic    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    word_list = nltk.word_tokenize(cleared_text.lower())\n",
    "    lemmatized_output = ' '.join([wnl.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "morph = MorphAnalyzer()\n",
    "wnl = WordNetLemmatizer()\n",
    "def lemmatize(text):\n",
    "    cleared_text = re.sub(r'[^a-zA-Z]',' ',text)\n",
    "    words = cleared_text.lower().split()     \n",
    "    res = list()\n",
    "    for word in words:\n",
    "        p = morph.parse(word)[0]\n",
    "        res.append(p.normal_form)\n",
    "    return \" \".join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f=df[:10000]\n",
    "lemmatized_text = df['text'].apply(lambda x: lemmatize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0         explanation why the edits made under my userna...\n",
      "1         d aww he matches this background colour i m se...\n",
      "2         hey man i m really not trying to edit war it s...\n",
      "3         more i can t make any real suggestions on impr...\n",
      "4         you sir are my hero any chance you remember wh...\n",
      "                                ...                        \n",
      "159566    and for the second time of asking when your vi...\n",
      "159567    you should be ashamed of yourself that is a ho...\n",
      "159568    spitzer umm theres no actual article for prost...\n",
      "159569    and it looks like it was actually you who put ...\n",
      "159570    and i really don t think you understand i came...\n",
      "Name: text, Length: 159571, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pd.DataFrame(lemmatized_text), df['toxic'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((127656, 1), (31915, 1))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((95742, 1), (31914, 1))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text\n",
      "108095  august utc let s wait until the abs release th...\n",
      "47135   de neville hi ealdgyth sorry i didn t know you...\n",
      "124803  the deletion of the term bulgarian in this art...\n",
      "150942    you should read the external linking policy t c\n",
      "155739  but as cessna mikoyan gurevich and piper to na...\n",
      "155094  there is no policy basis for the block your id...\n",
      "17459   btw i think i am free to see what what you do ...\n",
      "140599  you seem to be one of the more well intentione...\n",
      "128110  regarding edits made during november utc to sh...\n",
      "93528   the reason that morocco is not a member of the...\n"
     ]
    }
   ],
   "source": [
    "print(X_train[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf1 = TfidfVectorizer(stop_words=stopwords) \n",
    "train_tf_idf1 = count_tf_idf1.fit_transform(X_train['text'].values)\n",
    "val_tf_idf1 = count_tf_idf1.transform(X_val['text'].values)\n",
    "test_tf_idf1 = count_tf_idf1.transform(X_test['text'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 122599)\t0.07511400185807228\n",
      "  (0, 119484)\t0.1338926825446641\n",
      "  (0, 117563)\t0.24329989407646166\n",
      "  (0, 116507)\t0.09230487855318296\n",
      "  (0, 108902)\t0.2113729221421455\n",
      "  (0, 104358)\t0.2706825445008082\n",
      "  (0, 104293)\t0.12314212384350881\n",
      "  (0, 101955)\t0.19527788280633993\n",
      "  (0, 96604)\t0.11877926965493606\n",
      "  (0, 88825)\t0.17398693284943034\n",
      "  (0, 78600)\t0.17824452426583676\n",
      "  (0, 77958)\t0.12488951331058483\n",
      "  (0, 71136)\t0.2589678156464524\n",
      "  (0, 68231)\t0.19527788280633993\n",
      "  (0, 67202)\t0.22364778159232748\n",
      "  (0, 66362)\t0.12863370851908837\n",
      "  (0, 60232)\t0.08414483628745646\n",
      "  (0, 46622)\t0.1364524273630153\n",
      "  (0, 43775)\t0.1480168282163967\n",
      "  (0, 43228)\t0.2416912867620861\n",
      "  (0, 38139)\t0.16871519983019093\n",
      "  (0, 35763)\t0.22959011973986415\n",
      "  (0, 33032)\t0.1967738174472053\n",
      "  (0, 30150)\t0.15006036546985846\n",
      "  (0, 16212)\t0.12943096181466748\n",
      "  :\t:\n",
      "  (31914, 41095)\t0.11734710357808806\n",
      "  (31914, 39475)\t0.13236863736961532\n",
      "  (31914, 38253)\t0.10475477673414987\n",
      "  (31914, 38242)\t0.09293304930466588\n",
      "  (31914, 38166)\t0.17640870926923927\n",
      "  (31914, 37853)\t0.06610093165170884\n",
      "  (31914, 36628)\t0.05779149393484411\n",
      "  (31914, 33412)\t0.3105235937852812\n",
      "  (31914, 33320)\t0.05450308572611732\n",
      "  (31914, 32359)\t0.08445741155818377\n",
      "  (31914, 31125)\t0.11015481470821573\n",
      "  (31914, 27658)\t0.13203078611922217\n",
      "  (31914, 27650)\t0.0756435692108463\n",
      "  (31914, 23853)\t0.1066432664576204\n",
      "  (31914, 21928)\t0.08435237983244553\n",
      "  (31914, 21564)\t0.06954997720916752\n",
      "  (31914, 20561)\t0.13614907602807333\n",
      "  (31914, 20129)\t0.14931294550485305\n",
      "  (31914, 17188)\t0.09410694536491482\n",
      "  (31914, 13585)\t0.08890574056110669\n",
      "  (31914, 5663)\t0.08320469471576337\n",
      "  (31914, 5331)\t0.07149845907023726\n",
      "  (31914, 3628)\t0.11965655496676991\n",
      "  (31914, 3578)\t0.07047775553408288\n",
      "  (31914, 1198)\t0.0642681940326296\n"
     ]
    }
   ],
   "source": [
    "print(test_tf_idf1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_TestModels = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/metrics/classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]best_param_C= 1 ; best_max_iter_param= 10 ; best_class_weight_param= balanced ; best_f1= 0.7351589781290205\n"
     ]
    }
   ],
   "source": [
    "best_param_C = 0\n",
    "best_max_iter_param = 0\n",
    "best_class_weight_param = 0\n",
    "best_f1 = 0\n",
    "\n",
    "for param_C in (0.0001, 0.001, 0.01, 0.1, 1):\n",
    "    for max_iter_param in range(10, 1000, 50):\n",
    "        for class_weight_param in (None, 'balanced'):\n",
    "            model = LogisticRegression(C=param_C, max_iter=max_iter_param, class_weight=class_weight_param, verbose=2)\n",
    "            model.fit(train_tf_idf1, y_train)\n",
    "            f1 = f1_score(y_val, model.predict(val_tf_idf1))\n",
    "            if f1 > best_f1:\n",
    "                best_param_C = param_C\n",
    "                best_max_iter_param = max_iter_param\n",
    "                best_class_weight_param = class_weight_param\n",
    "                best_f1 = f1\n",
    "print('best_param_C=',best_param_C,'; best_max_iter_param=',best_max_iter_param,'; best_class_weight_param=',\n",
    "      best_class_weight_param,'; best_f1=',best_f1)            \n",
    "tmp = pd.DataFrame([[model, best_f1]])\n",
    "df_TestModels = df_TestModels.append(tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\"> Добавил поиск гиперпараметров. Лучше результаты с параметрами: best_param_C= 1 ; best_max_iter_param= 10 ; best_class_weight_param= balanced ; best_f1= 0.0.7351589781290205\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibLinear]F1= 0.7373612823674477\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=1, max_iter=10, class_weight='balanced', verbose=2)\n",
    "model.fit(train_tf_idf1, y_train)\n",
    "f1 = f1_score(y_test, model.predict(test_tf_idf1))\n",
    "print('F1=',f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_f1 = 0\n",
    "best_n_neighbor=0\n",
    "for n_neighbor in range(2,9,2):\n",
    "    model = KNeighborsClassifier(n_neighbors=n_neighbor)\n",
    "    model.fit(train_tf_idf1, y_train) \n",
    "    predict_test = model.predict(test_tf_idf1)\n",
    "    f1 = f1_score(y_test, model.predict(test_tf_idf1))\n",
    "    if f1 > best_f1:\n",
    "        best_n_neighbor = n_neighbor\n",
    "        best_f1 = f1\n",
    "    print(n_neighbor,\" F1:\", f1)\n",
    "\n",
    "tmp = pd.DataFrame([[model, best_f1]])\n",
    "df_TestModels = df_TestModels.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Слишком низкое качество прогноза."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf_idf1 = train_tf_idf1[:3500]\n",
    "y_train = y_train[:3500]\n",
    "val_tf_idf1 = val_tf_idf1[:3500]\n",
    "y_val = y_val[:3500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=90, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5436507936507936\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5515873015873016\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=110, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5443786982248522\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=120, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5343811394891944\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=130, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5372549019607843\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=140, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5296442687747035\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=150, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5312500000000001\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=160, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5252918287937743\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=170, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5307692307692308\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=14,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=180, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5240847784200385\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=90, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5338645418326694\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5458089668615984\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=110, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5444015444015444\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=120, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5394990366088633\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=130, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5287356321839081\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=140, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5338491295938105\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=150, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5240847784200385\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=160, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5295238095238096\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=170, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.523719165085389\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=16,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=180, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5273069679849341\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=90, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5346534653465347\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5546875\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=110, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5401174168297456\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=120, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5440313111545988\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=130, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5366795366795367\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=140, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5293005671077504\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=150, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5263157894736843\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=160, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5223880597014925\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=170, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5278810408921933\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=18,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=180, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5176908752327747\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=90, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5234375\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5193798449612402\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=110, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5232558139534884\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=120, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5269230769230769\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=130, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5267175572519085\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=140, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5215759849906191\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=150, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5215759849906191\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=160, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.516728624535316\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=170, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5138632162661738\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=20,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=180, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5119705340699816\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=90, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5468750000000001\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5372549019607843\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=110, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5261121856866539\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=120, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5220729366602688\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=130, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5285171102661597\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=140, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5290806754221389\n",
      "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
      "               importance_type='split', learning_rate=0.17, max_depth=22,\n",
      "               metric='f1', min_child_samples=20, min_child_weight=0.001,\n",
      "               min_split_gain=0.0, n_estimators=150, n_jobs=-1, num_leaves=31,\n",
      "               objective=None, random_state=None, reg_alpha=0.0, reg_lambda=0.0,\n",
      "               silent=True, subsample=1.0, subsample_for_bin=200000,\n",
      "               subsample_freq=0) :  0.5269016697588126\n"
     ]
    }
   ],
   "source": [
    "best_f1 = 0\n",
    "best_n_estimators = 0\n",
    "best_m_depth = 0\n",
    "n_estimators = 6\n",
    "max_depth = 9\n",
    "for maxdepth in range(14, 24, 2):\n",
    "    for nestimators in range(90, 190, 10):\n",
    "        model = LGBMClassifier(boosting_type='gbdt', #num_leaves=1200,\n",
    "                                    learning_rate=0.17, n_estimators=nestimators, max_depth=maxdepth,metric='f1')\n",
    "        model.fit(train_tf_idf1, y_train) \n",
    "        f1 = f1_score(y_val, model.predict(val_tf_idf1))\n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_n_estimators = n_estimators\n",
    "            best_m_depth = max_depth\n",
    "        print(model,': ',f1)\n",
    "print('best_f1 =', f1,'best_n_estimators =', best_n_estimators,  'best_m_depth =', best_m_depth)\n",
    "tmp = pd.DataFrame([[maxdepth, best_f1]])\n",
    "df_TestModels = df_TestModels.append(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тоже низкое качество прогноза."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_TestModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучшие результаты показала модель LogisticRegression f1= 0.7373612823674477 с параметрами: C= 1 ; max_iter = 10 ; class_weight = balanced. KNeighborsClassifier лучший результат F1: 0.2848114598693912.  LGBMClassifier F1= 0.6148771235362032.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Чек-лист проверки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [x]  Jupyter Notebook открыт\n",
    "- [x]  Весь код выполняется без ошибок\n",
    "- [x]  Ячейки с кодом расположены в порядке исполнения\n",
    "- [x]  Данные загружены и подготовлены\n",
    "- [x]  Модели обучены\n",
    "- [x]  Значение метрики *F1* не меньше 0.75\n",
    "- [x]  Выводы написаны"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
